{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import wget\n",
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizer\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "from transformers import DPRQuestionEncoder, DPRQuestionEncoderTokenizer\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "\n",
        "# You can also use this section to suppress warnings generated by your code:\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "dxABTJiNrVR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper Functions"
      ],
      "metadata": {
        "id": "fBnHRtjVrbIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tsne_plot(data):\n",
        "    # Apply t-SNE to reduce to 3D\n",
        "    tsne = TSNE(n_components=3, random_state=42,perplexity=data.shape[0]-1)\n",
        "    data_3d = tsne.fit_transform(data)\n",
        "\n",
        "    # Plotting\n",
        "    fig = plt.figure(figsize=(10, 7))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Assign colors for each point based on its index\n",
        "    num_points = len(data_3d)\n",
        "    colors = plt.cm.tab20(np.linspace(0, 1, num_points))\n",
        "\n",
        "    # Plot scatter with unique colors for each point\n",
        "    for idx, point in enumerate(data_3d):\n",
        "        ax.scatter(point[0], point[1], point[2], label=str(idx), color=colors[idx])\n",
        "\n",
        "    # Adding labels and titles\n",
        "    ax.set_xlabel('TSNE Component 1')\n",
        "    ax.set_ylabel('TSNE Component 2')\n",
        "    ax.set_zlabel('TSNE Component 3')\n",
        "    plt.title('3D t-SNE Visualization')\n",
        "    plt.legend(title='Input Order')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4OwNb26YreD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = 'companyPolicies.txt'\n",
        "url = 'https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/6JDbUb_L3egv_eOkouY71A.txt'\n",
        "\n",
        "# Use wget to download the file\n",
        "wget.download(url, out=filename)\n",
        "print('file downloaded')"
      ],
      "metadata": {
        "id": "a9l54Qj-tJwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_and_split_text(filename):\n",
        "    with open(filename, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    # Split the text into paragraphs (simple split by newline characters)\n",
        "    paragraphs = text.split('\\n')\n",
        "    # Filter out any empty paragraphs or undesired entries\n",
        "    paragraphs = [para.strip() for para in paragraphs if len(para.strip()) > 0]\n",
        "    return paragraphs\n",
        "\n",
        "# Read the text file and split it into paragraphs\n",
        "paragraphs = read_and_split_text('companyPolicies.txt')\n",
        "paragraphs[0:10]"
      ],
      "metadata": {
        "id": "aPOWbDs0uttV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(4):\n",
        "    print(f\"sample: {i} paragraph: {paragraphs[i]} \\n\" )"
      ],
      "metadata": {
        "id": "dtqHVFBlvBqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "context_tokenizer = DPRContextEncoderTokenizer.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')\n",
        "context_tokenizer"
      ],
      "metadata": {
        "id": "rllLC36VwK3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_encoder = DPRContextEncoder.from_pretrained('facebook/dpr-ctx_encoder-single-nq-base')"
      ],
      "metadata": {
        "id": "onwrHNf4xbtN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shuffling samples so that the samples are not ordered based on the category they belong to\n",
        "random.shuffle(paragraphs)"
      ],
      "metadata": {
        "id": "znkEw_C4yG0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_contexts(text_list):\n",
        "    # Encode a list of texts into embeddings\n",
        "    embeddings = []\n",
        "    for text in text_list:\n",
        "        inputs = context_tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=256)\n",
        "        outputs = context_encoder(**inputs)\n",
        "        embeddings.append(outputs.pooler_output)\n",
        "    return torch.cat(embeddings).detach().numpy()\n",
        "\n",
        "# you would now encode these paragraphs to create embeddings.\n",
        "context_embeddings = encode_contexts(paragraphs)"
      ],
      "metadata": {
        "id": "XIrw73ZByQuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "# Convert list of numpy arrays into a single numpy array\n",
        "embedding_dim = 768  # This should match the dimension of your embeddings\n",
        "context_embeddings_np = np.array(context_embeddings).astype('float32')\n",
        "\n",
        "# Create a FAISS index for the embeddings\n",
        "index = faiss.IndexFlatL2(embedding_dim)\n",
        "index.add(context_embeddings_np)  # Add the context embeddings to the index"
      ],
      "metadata": {
        "id": "LU0ZCbL_0rB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load DPR question encoder and tokenizer\n",
        "question_encoder = DPRQuestionEncoder.from_pretrained('facebook/dpr-question_encoder-single-nq-base')\n",
        "question_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained('facebook/dpr-question_encoder-single-nq-base')"
      ],
      "metadata": {
        "id": "uM2VbL0V4-oa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_relevant_contexts(question, question_tokenizer, question_encoder, index, k=5):\n",
        "    \"\"\"\n",
        "    Searches for the most relevant contexts to a given question.\n",
        "\n",
        "    Returns:\n",
        "    tuple: Distances and indices of the top k relevant contexts.\n",
        "    \"\"\"\n",
        "    # Tokenize the question\n",
        "    question_inputs = question_tokenizer(question, return_tensors='pt')\n",
        "\n",
        "    # Encode the question to get the embedding\n",
        "    question_embedding = question_encoder(**question_inputs).pooler_output.detach().numpy()\n",
        "\n",
        "    # Search the index to retrieve top k relevant contexts\n",
        "    D, I = index.search(question_embedding, k)\n",
        "\n",
        "    return D, I"
      ],
      "metadata": {
        "id": "OAswEkVt5pbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
        "model.generation_config.pad_token_id = tokenizer.pad_token_id"
      ],
      "metadata": {
        "id": "KL60iXga8T2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(question, contexts):\n",
        "    # Concatenate the retrieved contexts to form the input to GPT2\n",
        "    input_text = question + ' ' + ' '.join(contexts)\n",
        "    inputs = tokenizer(input_text, return_tensors='pt', max_length=1024, truncation=True)\n",
        "\n",
        "    # Generate output using GPT2\n",
        "    summary_ids = model.generate(inputs['input_ids'], max_new_tokens=50, min_length=40, length_penalty=2.0,\n",
        "                                 num_beams=4, early_stopping=True,pad_token_id=tokenizer.eos_token_id)\n",
        "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "3dFcntKH94US"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what is mobile policy?\"\n",
        "\n",
        "_,I =search_relevant_contexts(question, question_tokenizer, question_encoder, index, k=5)\n",
        "\n",
        "print(f\"paragraphs indexs {I}\")"
      ],
      "metadata": {
        "id": "b6vLA5IPCH47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_contexts = [paragraphs[idx] for idx in I[0]]\n",
        "print(f\"top_contexts {top_contexts}\")"
      ],
      "metadata": {
        "id": "wFaQxgKnCoaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume `I[0]` contains indices of top contexts from the retrieval step\n",
        "answer = generate_answer(question, top_contexts)\n",
        "print(\"Generated Answer:\", answer)"
      ],
      "metadata": {
        "id": "EdKFsmmTC-bu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}